{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /opt/conda/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (0.11)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_deprecate_positional_args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d8cf8a7554af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/impute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMissingIndicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_knn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNNImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_deprecate_positional_args'"
     ]
    }
   ],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unknown_to_nan(df, keys_dict):\n",
    "    \"\"\"\n",
    "    Replaces given keys from keys_dict to np.nan in df\n",
    "    \n",
    "    Input:\n",
    "        df (DataFrame): Dataset for which keys to need to be converted nan \n",
    "        keys_dict: dictionary of column names with keys that need to be converted to nan\n",
    "\n",
    "    \"\"\"\n",
    "    # Search for unknown keys in each column and replace\n",
    "\n",
    "    for column_name in keys_dict:\n",
    "        if column_name in df.columns:       \n",
    "            keys_values = keys_dict[column_name]\n",
    "            for key in keys_values:\n",
    "                if key == 'X' or key == 'XX':\n",
    "                    key = str(key)\n",
    "                else:\n",
    "                    key = int(key)\n",
    "                \n",
    "                df[column_name].replace(key, np.NaN, inplace=True)\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "def drop_columns(df, missing_columns, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Columns with more than 30% missing values will be dropped.\n",
    "\n",
    "    INPUT:\n",
    "    - df: DataFrame to be cleaned\n",
    "    - missing_columns: count of missing values per column\n",
    "    - threshold: threshold as decision criteria for dropping (default 0.3)\n",
    "\n",
    "    OUTPUT:\n",
    "    - clean_df: DataFrame with a smaller percentage of missing values than threshold\n",
    "    - drop_null_cols: List of columns that were dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    column_missing_values_perc = missing_columns/len(df)                \n",
    "    drop_null_cols = list(column_missing_values_perc[column_missing_values_perc>threshold].index)\n",
    "    clean_df = df.drop(drop_null_cols, axis=1)\n",
    "    \n",
    "    return clean_df, drop_null_cols\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "def drop_rows(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Rows with more than 30% missing values will be dropped.\n",
    "\n",
    "    INPUT:\n",
    "    - df: DataFrame to be cleaned\n",
    "    - missing_rows: count of missing values per row\n",
    "    - threshold: threshold as decision criteria for dropping (default 0.3)\n",
    "\n",
    "    OUTPUT:\n",
    "    - clean_df: DataFrame with a smaller percentage of missing values than threshold\n",
    "    \"\"\"\n",
    "    missing_rows= df.isnull().sum(axis=1)\n",
    "    clean_df = df[missing_rows / df.shape[1] <= threshold]\n",
    "    \n",
    "    return clean_df\n",
    "    \n",
    "#########################################################################################################    \n",
    "    \n",
    "def clean_data(df, unknown_values_dict, drop_cols, test_data=False):\n",
    "    \"\"\"\n",
    "    Cleans the dataframe: \n",
    "    - Convert unknowns to NaN\n",
    "    - Drop columns and rows with high amount of mising values and further unvaluable columns ('LNR')\n",
    "    - Convert columns 'EINGEFUEGT_AM' and 'OST_WEST_KZ'\n",
    "    \n",
    "    Input:\n",
    "        df: Dataset that needs to be cleaned up \n",
    "        unknown_values_dict: Dictionary containing column names with keys that need to be converted to nan\n",
    "        drop_cols: list of columns to be dropped\n",
    "        test_data (True/False): True if dataframe is mailout_test dataframe, here rows shouldn't be dropped\n",
    "\n",
    "    Output:\n",
    "        clean_df: Clean dataset\n",
    "    \"\"\"\n",
    "    # Convert unknowns to NaN\n",
    "    convert_unknown_to_nan(df, unknown_values_dict)\n",
    "    \n",
    "    # drop columns with more than 30% missing values\n",
    "    clean_df = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    # drop columns with unique identifiers (here: \"LNR\")\n",
    "    clean_df = clean_df.drop(['LNR'], axis=1)\n",
    "    \n",
    "    # drop rows with more than 30% missing values\n",
    "    if test_data==False:\n",
    "        clean_df = drop_rows(clean_df, threshold=0.3)\n",
    "                             \n",
    "    # Convert columns 'EINGEFUEGT_AM' and 'OST_WEST_KZ'\n",
    "    clean_df['EINGEFUEGT_AM'] = pd.to_datetime(clean_df['EINGEFUEGT_AM'], format='%Y-%m-%d ').dt.year\n",
    "    \n",
    "    clean_df['OST_WEST_KZ'].replace('O', 0, inplace=True)\n",
    "    clean_df['OST_WEST_KZ'].replace('W', 1, inplace=True)\n",
    "    clean_df['OST_WEST_KZ'] = pd.to_numeric(clean_df['OST_WEST_KZ'], errors = 'coerce')\n",
    "    \n",
    "\n",
    "    return clean_df\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "class Log1pTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms skewed continuous data to log to get normal distributed data\n",
    "    \n",
    "    Input: \n",
    "    columns: List of columns to be log-transformed (not mandatory)\n",
    "    \n",
    "    Output:\n",
    "    X_transformed: Dataframe with log-transformed data\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for column in self.columns:\n",
    "            try:\n",
    "                X_transformed[column] = np.log1p(X_transformed[column])\n",
    "            except ValueError:\n",
    "                X_transformed[column] = np.NaN\n",
    "        return X_transformed\n",
    "    \n",
    "#########################################################################################################\n",
    "\n",
    "# Custom Imputer function\n",
    "class CustomImputer(TransformerMixin):\n",
    "    def __init__(self, columns=None, strategy='mean'):\n",
    "        self.cols = columns\n",
    "        self.strategy = strategy\n",
    "            \n",
    "    def transform(self, df):\n",
    "        X = df.copy()\n",
    "        impute = SimpleImputer(strategy=self.strategy)\n",
    "        \n",
    "        if self.cols == None:\n",
    "            self.cols = list(X.columns)\n",
    "        for col in self.cols:\n",
    "            if X[col].dtype == np.dtype('O') : \n",
    "                X[col].fillna(X[col].value_counts().index[0], inplace=True)\n",
    "            else : X[col] = impute.fit_transform(X[[col]])\n",
    "        return X\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "\n",
    "def int_to_str(column):\n",
    "    \"\"\"\n",
    "    Custom function to convert integer to string  \n",
    "    \"\"\"\n",
    "    return column.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and basic evaluation\n",
    "- read in csv files `Udacity_AZDIAS_052018.csv` and `Udacity_CUSTOMERS_052018.csv`\n",
    "- basic data evaluation: shape, head, info, describe, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic data evaluation AZDIAS\n",
    "print(azdias.shape)\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(azdias.head())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(azdias.info())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(azdias.describe())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(azdias.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic data evaluation CUSTOMERS\n",
    "print(customers.shape)\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(customers.head())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(customers.info())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(customers.describe())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(customers.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Steps\n",
    "\n",
    "- Explore features (Differences in columns)\n",
    "\n",
    "- Check data types\n",
    "\n",
    "- Explore features using `DIAS Attributes - Values 2017.xlsx`\n",
    "\n",
    "- Missing Values \n",
    "    - Convert Unknown to NaN\n",
    "    - Columns with NaN (distribution of % missing --> decide what to drop)\n",
    "    - Rows with NaN (distribution of % missing --> decide what to drop)\n",
    "\n",
    "- Datatypes: Classify attribute types (https://www.mygreatlearning.com/blog/types-of-data/)\n",
    "    - Quantitative Attributes (Numerical)\n",
    "        - Discrete / Continous data\n",
    "    - Qualitative Attributes (Categorical)\n",
    "    - Impute NaN for each data type?\n",
    "\n",
    "- Encoding / Standardization (OneHotEncoder / StandardScaler)\n",
    "\n",
    "### Explore Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differences in columns\n",
    "azdias_cols = set(azdias.columns)\n",
    "customers_cols = set(customers.columns)\n",
    "customers_cols - azdias_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 columns in the customers data set not included in azdias data set: 'CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types azdias\n",
    "azdias.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types customers\n",
    "customers.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Categorical features in AZDIAS:\\n', azdias.select_dtypes(['object']).head(5))\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print('Categorical features in CUSTOMERS:\\n', customers.select_dtypes(['object']).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 categorical features in the azdias data set but 8 categorical featrues in the customers data set. \n",
    "\n",
    "The feature information to these categorical features are the following:\n",
    "- CAMEO_DEU_2015 / CAMEO_DEUG_2015 / CAMEO_INTL_2015: These describe the sociodemographic background of a person or household based on the CAMEO classification system.\n",
    "- D19_LETZTER_KAUF_BRANCHE: Industry in which last purchase was made\n",
    "- EINGEFUEGT_AM: Date where record was added \n",
    "- OST_WEST_KZ: Inidcates whether a house is located in West Germany / Federal Republik of Germany / FRG (W) or East Germany / German Democratic Republik / GDR (O)\n",
    "- PRODUCT_GROUP: Product Group where the transactional activity has taken place\n",
    "- CUSTOMER_GROUP: Classification of the customer as MULTI_BUYER or SINGLE_BUYER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Features using DIAS Attributes - Values 2017.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore features using DIAS Attributes - Values 2017.xlsx\n",
    "attr_values = pd.read_excel('DIAS Attributes - Values 2017.xlsx', usecols='B:E', dtype='str')\n",
    "attr_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several values in the features that have the meaning \"unknown\". These should be replaced by NaN in the dataset in order to perform a correct estimation and cleanup of missing values.\n",
    "Among numerical characteristics, there are many whose values have natural, ordered categories. These need to be identified and classified as ordinal or categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify values with meaning unknown\n",
    "unknown_values = attr_values[attr_values[\"Meaning\"].isin([\"unknown\",\"unknown / no main age detectable\"])]\n",
    "unknown_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "- Convert Unknown to NaN\n",
    "- Columns with NaN (distribution of % missing --> decide what to drop)\n",
    "- Rows with NaN (distribution of % missing --> decide what to drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of missing attribute-value combinations\n",
    "unknown_values_dict = {}\n",
    "for _, row in unknown_values.iterrows():\n",
    "    key = row[\"Attribute\"]\n",
    "    unknown_values_dict[key] = row[\"Value\"].split(\", \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually add missing codes from other/similar attributes \n",
    "unknown_values_dict[\"CAMEO_INTL_2015\"] = ['XX']\n",
    "unknown_values_dict[\"CAMEO_DEUG_2015\"] = ['X','XX']\n",
    "unknown_values_dict[\"CAMEO_DEU_2015\"] =['XX']\n",
    "unknown_values_dict[\"GEBURTSJAHR\"] = ['0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values before converted to nan\n",
    "azdias.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df into samples to reduce data amount (to prevent Jupyter Kernel from dying)\n",
    "azdias_1 = azdias.iloc[:300000]\n",
    "azdias_2 = azdias.iloc[300000:600000]\n",
    "azdias_3 = azdias.iloc[600000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert unknowns to nan\n",
    "convert_unknown_to_nan(azdias_1, unknown_values_dict)\n",
    "#convert_unknown_to_nan(azdias_2, unknown_values_dict)\n",
    "#convert_unknown_to_nan(azdias_3, unknown_values_dict)\n",
    "\n",
    "# concat samples to one df\n",
    "#frames = [azdias_1, azdias_2, azdias_3]\n",
    "#azdias = pd.concat(frames)\n",
    "\n",
    "# Trying to convert the whole azdias df the kernel of Jupyter died. Here just a sample is cleaned and used for the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values AFTER converted to nan\n",
    "azdias_1.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only columns with missing values\n",
    "column_missing_values = azdias_1.isnull().sum().sort_values(ascending=False)\n",
    "column_missing_values = column_missing_values[column_missing_values > 0]\n",
    "\n",
    "print(\"There are {} columns with missing values.\".format(len(column_missing_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 columns with highest count of missing values\n",
    "column_missing_values.head(100).plot.bar(figsize=(16,8))\n",
    "plt.title('Columns and number of missing values')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Number of missing values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of missing values\n",
    "column_missing_values_perc = column_missing_values/len(azdias_1) * 100\n",
    "column_missing_values_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 100 columns with highest share of missing values in %\n",
    "column_missing_values_perc.head(100).plot.bar(figsize=(16,8))\n",
    "plt.title('Top 100 missing columns')\n",
    "plt.xlabel(\"Column\")\n",
    "plt.ylabel(\"Share of missing values in %\")\n",
    "plt.show\n",
    "\n",
    "print(\"9 columns have 50% or more missing values. Most of the columns have less than 20% missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of missing values\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(column_missing_values_perc, bins=np.linspace(5,100))\n",
    "plt.title('Distribution of missing data in columns')\n",
    "plt.xticks(np.arange(0, 100, 5))\n",
    "plt.ylabel('# of columns')\n",
    "plt.xlabel('% of missing data')\n",
    "\n",
    "print(column_missing_values_perc.describe())\n",
    "print('----------------------------------------------------------------------------------------------------------------------')\n",
    "print(\"On average, columns have a missing share of {}%.\".format(round(column_missing_values_perc.mean(),1)))\n",
    "print(\"97% of columns have a missing share of {}% or less.\".format(round(column_missing_values_perc.quantile(0.97),1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with >30% missing values\n",
    "azdias_dropped_cols, drop_null_cols = drop_columns(azdias_1, column_missing_values, 0.3)\n",
    "\n",
    "print(\"{} columns have more than 30% missing values and will be will be dropped: {}\".format(len(drop_null_cols),drop_null_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in Rows:\n",
    "missing_rows = azdias_1.isnull().sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(missing_rows, bins=np.linspace(min(missing_rows), max(missing_rows)+1))\n",
    "plt.title('Distribution of missing values in rows');\n",
    "plt.xticks(np.arange(min(missing_rows), max(missing_rows)+1, 10))\n",
    "plt.ylabel('# of rows')\n",
    "plt.xlabel('# of missing values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_dropped_rows = drop_rows(azdias_1, 0.3)\n",
    "\n",
    "print(\"In total {}% of rows were dropped having >30% missing values.\".format(round(100-100*azdias_dropped_rows.shape[0]/len(azdias_1),0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_sample = azdias.iloc[:10000]\n",
    "clean_azdias_sample = clean_data(azdias_sample, unknown_values_dict, drop_null_cols, test_data=False)\n",
    "\n",
    "print(\"{} rows are dropped and {} columns.\".format(azdias_sample.shape[0]-clean_azdias_sample.shape[0], azdias_sample.shape[1]-clean_azdias_sample.shape[1]))\n",
    "clean_azdias_sample.head()\n",
    "\n",
    "# Trying to clean the whole azdias df the kernel of Jupyter died. Here just a sample is cleaned and used for the next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_sample = customers.iloc[:10000]\n",
    "clean_customers_sample = clean_data(customers_sample.drop(['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP'], axis=1), unknown_values_dict, drop_null_cols, test_data=False)\n",
    "\n",
    "print(\"{} rows are dropped and {} columns.\".format(customers_sample.shape[0]-clean_customers_sample.shape[0], customers_sample.shape[1]-clean_customers_sample.shape[1]))\n",
    "clean_customers_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify attribute types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of numerical columns:\n",
    "num_cols = clean_azdias_sample.select_dtypes(['float64', 'int64']).columns\n",
    "\n",
    "binary_cols = []\n",
    "for col in num_cols:\n",
    "    n_unique = clean_azdias_sample[col].dropna().nunique()\n",
    "    if n_unique == 2:\n",
    "        binary_cols.append(col)\n",
    "        \n",
    "discrete_cols = list(clean_azdias_sample.select_dtypes(['int64']).columns)\n",
    "continuous_cols = list(set(num_cols) - set(discrete_cols) - set(binary_cols))\n",
    "\n",
    "# total number of categorical columns:\n",
    "cat_cols = list(clean_azdias_sample.select_dtypes(['object']).columns)\n",
    "\n",
    "print(\"Number of numerical columns: {} \\nNumber of categorical columns: {}\\n\".format(len(num_cols), len(cat_cols)))\n",
    "\n",
    "print(\"List of continuous columns: {} \\n\\nList of binary columns: {} \\n\\nList of discrete columns: {} \\n\\nList of categorical columns: {}\".format(continuous_cols, binary_cols, discrete_cols, cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute, encode and standardize features\n",
    "- Quantitative Attributes (Numerical)\n",
    "    - Discrete: Missing values of discrete features will be imputed based on the median value and standardized using StandardScaler()\n",
    "    - Continous: All continuous features will be imputed based on median. Because there are some discrete features in the list due to their float datatype, the decimal should stay 0 after imputation. Skewed features will be log-transformed first and imputed based on the median value to avoid bias. All continuous features will be standardized with StandardScaler()\n",
    "    - Binary: Missing values will be imputed based on the most common value.\n",
    "- Qualitative Attributes (Categorical)\n",
    "    - Categorical features will be one-hot encoded and then imputed on the most common value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete features:\n",
    "discrete_pipeline = Pipeline([\n",
    "    ('discrete_impute', SimpleImputer(strategy='median')),\n",
    "    ('discrete_scale', StandardScaler())\n",
    "])\n",
    "\n",
    "# Continuous features:\n",
    "# Check for skewness\n",
    "skewed_cols = []\n",
    "for col in continuous_cols:\n",
    "    try:\n",
    "        skewness = skew(clean_azdias_sample[col].dropna())\n",
    "        if abs(skewness) > 1:\n",
    "                        skewed_cols.append(col)\n",
    "    except:\n",
    "         pass\n",
    "\n",
    "#print(skewed_cols)\n",
    "\n",
    "# Skewed continuous features:\n",
    "\n",
    "skewed_pipeline = Pipeline([\n",
    "    ('skewed_transform', Log1pTransformer(skewed_cols)),\n",
    "    ('skewed_impute', SimpleImputer(strategy='median')),\n",
    "    ('skewed_scale', StandardScaler())\n",
    "])\n",
    "\n",
    "# Normal disctributed continuous features:\n",
    "normal_pipeline = Pipeline([\n",
    "    ('normal_impute', SimpleImputer(strategy='median')),\n",
    "    ('normal_scale', StandardScaler())\n",
    "])\n",
    "\n",
    "# Binary features:\n",
    "binary_pipeline = Pipeline([(\n",
    "    'binary_impute', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "\n",
    "# Categorical features:\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('binary_impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('string_conversion', FunctionTransformer(int_to_str, validate=False)),\n",
    "    ('encoding', OneHotEncoder(sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_pipeline.fit_transform(clean_azdias_sample[discrete_cols])\n",
    "\n",
    "skewed_pipeline.fit_transform(clean_azdias_sample[skewed_cols])\n",
    "\n",
    "normal_cols = list(set(continuous_cols)-set(skewed_cols))\n",
    "normal_pipeline.fit_transform(clean_azdias_sample[normal_cols])\n",
    "\n",
    "binary_pipeline.fit_transform(clean_azdias_sample[binary_cols])\n",
    "\n",
    "categorical_pipeline.fit_transform(clean_azdias_sample[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_pipeline.fit_transform(clean_customers_sample[discrete_cols])\n",
    "\n",
    "skewed_pipeline.fit_transform(clean_customers_sample[skewed_cols])\n",
    "\n",
    "normal_cols = list(set(continuous_cols)-set(skewed_cols))\n",
    "normal_pipeline.fit_transform(clean_customers_sample[normal_cols])\n",
    "\n",
    "binary_pipeline.fit_transform(clean_customers_sample[binary_cols])\n",
    "\n",
    "categorical_pipeline.fit_transform(clean_customers_sample[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply PCA\n",
    "- Plot cumulative explained variance (y) per number of components (x) -> decide how many components are necessary to explain the variance (variance treshhold somewhere between 80-95%)\n",
    "- Apply PCA with reduced number of components\n",
    "- Analyze the top three components and the columns with highest weight in each of the components --> gives idea about columns with highest importance/influence\n",
    "- Apply K-Means\n",
    "- Plot elbow curve to determine number of clusters\n",
    "- Apply PCA and K-Means with same params to customer data set\n",
    "- pickle cluster results\n",
    "- compare both datasets in terms of cluster-sizes: plot population_count and customers_count per cluster (in %)\n",
    "- Analyze overrepresentated and underrepresentated clusters from customer data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative variance per numer of components (AZDIAS)\n",
    "\n",
    "pca = PCA()\n",
    "pca_full_azdias = PCA.fit_transform(clean_azdias_sample)\n",
    "\n",
    "plt.plot(np.cumsum(PCA.explained_variance_ratio_))\n",
    "plt.title('Explained variance by principal components (AZDIAS)')\n",
    "plt.xlabel('Number of principle components')\n",
    "plt.ylabel('Explained variance')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
